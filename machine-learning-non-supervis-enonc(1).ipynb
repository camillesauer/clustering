{"cells":[{"metadata":{"_uuid":"5c25a75c9f3ea42bff4e090cb90bf3cde2341a98"},"cell_type":"markdown","source":"# Algorithmes de clustering - Non supervisé\n\nDans l'apprentissage automatique, les types d'apprentissage peuvent être classés en trois grandes catégories : \n\n1. L'apprentissage supervisé \n2. L'apprentissage non supervisé \n3. L'apprentissage semi-supervisé  \n\nLes algorithmes appartenant à la famille de l'apprentissage non supervisé n'ont aucune variable à prédire liée aux données. Au lieu d'avoir une sortie, les données n'ont qu'une entrée qui serait constituée de plusieurs variables qui décrivent les données. C'est là qu'intervient le regroupement.\n\nLa mise en cluster consiste à regrouper un ensemble d'objets de telle sorte que les objets d'un même cluster se ressemblent davantage les uns les autres qu'avec les objets d'autres clusters. La similarité est une mesure qui reflète la force de la relation entre deux objets de données. Le regroupement est principalement utilisé pour l'exploration de données. Elle est utilisée dans de nombreux domaines tels que le Machine Learning, la reconnaissance de formes, l'analyse d'images, la recherche d'informations, la bio-informatique, la compression de données et l'infographie."},{"metadata":{"_uuid":"46264f2aa942327fb740aa3fa54f5c3746f65f4d"},"cell_type":"markdown","source":"## K-Means \n\nIl existe de nombreux modèles de **clustering**. Nous allons passer en revue les plus populaires. Malgré sa simplicité, le **K-means** est largement utilisé pour le clustering dans de nombreuses applications de Dara Science, particulièrement utile si vous avez besoin de découvrir rapidement des aperçus à partir de données **non étiquetées**. Dans ce notebook, nous voyons comment utiliser le k-Means pour la segmentation de la clientèle."},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Question 1:\n- Importer numpy, pandas, matplotlib.pyplot, seaborn\n- Importer le fichier Mall_customers.csv et le stocker dans df\n- Afficher le nombre de lignes et de colonnes\n- Afficher les statistiques des variables\n- Afficher le typage des variables\n- Afficher les 10 première lignes de df"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline \n\nimport os\nimport warnings\n\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/ml-training-vlib/Mall_Customers.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2580bdf6c7764e83a99469fedde019c85fb45b63","trusted":true},"cell_type":"code","source":"df.info(), df.describe(), df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Question 2:\n- Renommer les variables \"Annual Income (k$)\" en 'Income' et 'Spending Score (1-100)' en 'Score'. Utiliser .rename() de pandas pour cela. https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.rename.html\n- Afficher un pairplit selon le genre (Homme - Femme) https://seaborn.pydata.org/generated/seaborn.pairplot.html\n- Que peut on dire sur la variable gender? Semble t'elle pertinente pour notre segmentation?"},{"metadata":{"_uuid":"a21ccf4b8663dfa37339cfba9af93ca800b6ebe8","trusted":true},"cell_type":"code","source":"df.rename(index=str, columns={'Annual Income (k$)': 'Income',\n                              'Spending Score (1-100)': 'Score'}, inplace=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"60429c30e21e07345322bb9990a372b52b313a47","trusted":true},"cell_type":"code","source":"# Let's see our data in a detailed way with pairplot\nX = df.drop(['CustomerID', 'Gender'], axis=1)\nsns.pairplot(df.drop('CustomerID', axis=1), hue='Gender', aspect=1.5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Réponse: Le graphique ci-dessus montre que le sexe n'a pas de rapport direct avec la segmentation de la clientèle. C'est pourquoi nous pouvons laisser tomber et passer à d'autres caractéristiques et c'est pourquoi nous allons désormais utiliser le paramètre X."},{"metadata":{},"cell_type":"markdown","source":"# Question 3:\n![](https://dendroid.sk/wp-content/uploads/2013/01/kmeansimg-scaled1000.jpg)\n\n- Importer Kmeans de sklearn. https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n- Nous voulons réaliser plusieurs Kmeans afin d'utiliser la règle du coude afin de définir le nombre de cluster. Réaliser 10 kmeans."},{"metadata":{"_uuid":"3e5302415ae26e1e564ed65fc4ed464b85aa2fea","trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\n\nclusters = []\n\nfor i in range(1, 11):\n    km = KMeans(n_clusters=i).fit(X)\n    clusters.append(km.inertia_)\n    \nfig, ax = plt.subplots(figsize=(12, 8))\nsns.lineplot(x=list(range(1, 11)), y=clusters, ax=ax)\nax.set_title('Searching for Elbow')\nax.set_xlabel('Clusters')\nax.set_ylabel('Inertia')\n\n# Annotate arrow\nax.annotate('Possible Elbow Point', xy=(3, 140000), xytext=(3, 50000), xycoords='data',          \n             arrowprops=dict(arrowstyle='->', connectionstyle='arc3', color='blue', lw=2))\n\nax.annotate('Possible Elbow Point', xy=(5, 80000), xytext=(5, 150000), xycoords='data',          \n             arrowprops=dict(arrowstyle='->', connectionstyle='arc3', color='blue', lw=2))\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"727664a2b0f592b6e4e4fed1114650c93949d9ba"},"cell_type":"markdown","source":"La méthode du coude nous dit de sélectionner le cluster lorsqu'il y a un changement significatif de l'inertie. Comme on peut le voir sur le graphique, on peut dire que cela peut être 3 ou 5. Voyons les deux résultats dans le graphique et décidons.\n\nPrincipe :Une stratégie simple pour identifier le nombre de classes consiste à faire varier K et surveiller l’évolution de l’inertie intra-classes W. L’idée est de visualiser le «coude» où l’adjonction d’une classe ne correspond à rien dans la structuration des données.\n\nPour la suite nous choisirons K = 3."},{"metadata":{},"cell_type":"markdown","source":"# Question 4:\n- Instancier la méthode avec les paramètres n_clusters = 3\n- Lancer un Kmeans avec K = 3\n- Afficher le scatter plot croisant \"Income\" et \"Score\" avec pour label (hue) \"Labels\""},{"metadata":{"_uuid":"b466a3ad8e491085577593d872e6c1085eecbf26","trusted":true},"cell_type":"code","source":"# 3 cluster\nkm3 = KMeans(n_clusters=3).fit(X)\n\nX['Labels'] = km3.labels_\nplt.figure(figsize=(12, 8))\nsns.scatterplot(X['Income'], X['Score'], hue=X['Labels'], \n                palette=sns.color_palette('hls', 3))\nplt.title('KMeans with 3 Clusters')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Question 5:\n- Instancier la méthode avec les paramètres n_clusters = 5\n- Lancer un Kmeans avec K = 5\n- Afficher le scatter plot croisant \"Income\" et \"Score\" avec pour label (hue) \"Labels\"\n- Utiliser random_state = 42 afin d'avoir tout le temps les mêmes résultats"},{"metadata":{"_uuid":"4ab6885ee7c04cceba940b5e46c09d77992e2ce7","trusted":true},"cell_type":"code","source":"# Let's see with 5 Clusters\nkm5 = KMeans(n_clusters=5, random_state = 42).fit(X)\n\nX['Labels'] = km5.labels_\nplt.figure(figsize=(12, 8))\nsns.scatterplot(X['Income'], X['Score'], hue=X['Labels'], \n                palette=sns.color_palette('hls', 5))\nplt.title('KMeans with 5 Clusters')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"50774078317f9606e034aeed8e0b02d0aea56065"},"cell_type":"markdown","source":"À en juger par les graphiques, on pourrait dire que 5 grappes semblent meilleures que les 3. Comme il s'agit d'un problème non supervisé, nous ne pouvons pas vraiment savoir avec certitude lequel est le meilleur dans la vie réelle, mais en examinant les données, on peut dire sans risque de se tromper que 5 est un bon choix. "},{"metadata":{},"cell_type":"markdown","source":"# Question 6:\nNous avons identifier des groupes cependant il faut analyser les différences entre ces groupes afin de pouvoir donner un nom plus explicite que \"Cluster N\". Pour cela nous allons réaliser des graphiques. Il est possible de comparer nos populations avec des tests statistiques afin de juger de la significativité des différences entre clusters.\n\n- Afficher le swarmplot entre \"Labels\" et \"Income\"\n- Afficher le swarmplot entre \"Labels\" et \"Score\"\n- Analyser les graphiques afin de trouver des noms à nos clusters"},{"metadata":{"_uuid":"02abec3aef3222ebd3f337767a01deb536cdbd58","trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(20,8))\nax = fig.add_subplot(121)\nsns.swarmplot(x='Labels', y='Income', data=X, ax=ax)\nax.set_title('Labels According to Annual Income')\n\nax = fig.add_subplot(122)\nsns.swarmplot(x='Labels', y='Score', data=X, ax=ax)\nax.set_title('Labels According to Scoring History')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Réponse: \nNous pouvons maintenant analyser en détail nos 5 groupes :\n- L'étiquette 0 correspond aux revenus élevés et dépenses faibles\n- L'étiquette 1 correspond aux revenus moyens et dépenses moyennes\n- L'étiquette 2 correspond aux revenu faibles et dépenses élevés\n- L'étiquette 3 correspond aux revenus élevés et dépenses élevés\n- L'étiquette 4 correspond aux revenu faibles et dépenses faibles"},{"metadata":{},"cell_type":"markdown","source":"## Hierarchical Clustering\n\n## Agglomerative\nNous nous pencherons sur une technique de clustering, à savoir la mise en cluster hiérarchique agglomérative. L'agglomération est l'approche ascendante qui est plus populaire que le regroupement par division. Nous utiliserons également le lien complet comme critère de liaison. \n\nLa classe de regroupement agglomératif nécessitera deux entrées :\n- n_clusters : Le nombre de grappes à former ainsi que le nombre de centroïdes à générer. \n- lien : Quel critère de liaison à utiliser. Le critère de lien détermine la distance à utiliser entre les ensembles d'observation. L'algorithme fusionnera les paires de grappes qui minimisent ce critère."},{"metadata":{},"cell_type":"markdown","source":"# Question 7:\n- Importer AgglomerativeClustering de sklearn. https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html\n- Instancier la méthode avec les paramètres n_clusters = 5\n- Afficher avec le scatterplot la relation \"Income\" et \"Score\" avec les labels associés ('Labels' - hue)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import AgglomerativeClustering \n\nagglom = AgglomerativeClustering(n_clusters=5).fit(X)\n\nX['Labels'] = agglom.labels_\nplt.figure(figsize=(12, 8))\nsns.scatterplot(X['Income'], X['Score'], hue=X['Labels'], \n                palette=sns.color_palette('hls', 5))\nplt.title('Agglomerative with 5 Clusters')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dendrogram pour Agglomerative Hierarchical Clustering\n\n![](https://miro.medium.com/max/2556/1*r1YriAFjwJokgcdtaodQAQ.png)\n\nN'oubliez pas qu'une matrice de distance contient la distance entre chaque point et chaque autre point d'un ensemble de données. Nous pouvons utiliser la fonction distance_matrix, qui nécessite deux entrées. N'oubliez pas que les valeurs de distance sont symétriques, avec une diagonale de 0. C'est une façon de s'assurer que votre matrice est correcte."},{"metadata":{},"cell_type":"markdown","source":"# Question 8:\n- Importer hierarchy de scipy - https://docs.scipy.org/doc/scipy/reference/cluster.hierarchy.html\n- Importer distance_matrix de scipy - https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance_matrix.html\n- Utiliser distance_matrix pour calculer la matrice des distances en les individus de X\n- Affihcer la matrice et vérifier que vous avez des 0 sur la diagonale"},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.cluster import hierarchy \nfrom scipy.spatial import distance_matrix \n\ndist = distance_matrix(X, X)\nprint(dist)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Question 9:\n- Utiliser hierarchy.linkage sur la matrice des distances calculées précédemment\n- Stocker dans Z"},{"metadata":{"trusted":true},"cell_type":"code","source":"Z = hierarchy.linkage(dist)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Un regroupement hiérarchique est généralement visualisé sous la forme d'un dendrogramme, comme le montre la cellule suivante. Chaque fusion est représentée par une ligne horizontale. La coordonnée y de la ligne horizontale est la similitude des deux groupes qui ont été fusionnés. En remontant de la couche inférieure vers le nœud supérieur, un dendrogramme nous permet de reconstruire l'histoire des fusions qui ont abouti à la mise en cluster représentée. "},{"metadata":{},"cell_type":"markdown","source":"# Question 10:\n- Afficherle dendogram de Z"},{"metadata":{"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(18, 50))\ndendro = hierarchy.dendrogram(Z, leaf_rotation=0, leaf_font_size=12, orientation='right')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Density Based Clustering (DBSCAN)\n\n![](https://miro.medium.com/max/1128/1*ejlV2WryiH4zGFP_KohEeA.png)\n\nLa plupart des techniques de regroupement traditionnelles, telles que les k-means, le regroupement hiérarchique, peuvent être utilisées pour regrouper des données sans supervision. \n\nCependant, lorsqu'elles sont appliquées à des tâches avec des clusters de forme arbitraire, ou des clusters à l'intérieur d'un cluster, les techniques traditionnelles peuvent ne pas permettre d'obtenir de bons résultats. En d'autres termes, les éléments d'un même groupe peuvent ne pas être suffisamment similaires ou les performances peuvent être médiocres. En outre, le clustering basé sur la densité localise les régions de haute densité qui sont séparées les unes des autres par des régions de faible densité. La densité, dans ce contexte, est définie comme le nombre de points dans un rayon donné.\n\nDans cette partie, l'accent sera mis sur la manipulation des données et des propriétés de DBSCAN et sur l'observation du clustering qui en résulte. De plus Kmeans force la création d'un nombre K de clusters. Ainsi tout les individus seront assignés à un regroupement, même les outliers... DBSCAN prend en compte ce type d'indivdus et va les isoler dans un autre cluster.\n\n### Modeling\nDBSCAN est l'acronyme de Density-Based Spatial Clustering of Applications with Noise. Cette technique est l'un des algorithmes de mise en cluster les plus courants qui fonctionne sur la base de la densité de l'objet. L'idée générale est que si un point particulier appartient à une cluster, il doit être proche de nombreux autres points de ce cluster.\n\nIl fonctionne sur la base de deux paramètres : Epsilon et points minimums  \n- Epsilon : Détermine un rayon précis qui, s'il comprend un nombre suffisant de points, est appelé zone dense  \n- minimumSamples : Détermine le nombre minimum de points de données que nous voulons dans un quartier pour définir un cluster."},{"metadata":{},"cell_type":"markdown","source":"# Question 11:\n- Importer DBSCAN\n- Utiliser les paramètres eps = 11 et min_samples = 6 puis fitté l'algorithme\n- Affihcer le scatterplot pour le croisement \"Income\", \"Score\" avec les labels des clusters\n- Analyse les résultats du DBSCAN"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import DBSCAN \n\ndb = DBSCAN(eps=11, min_samples=6).fit(X)\n\nX['Labels'] = db.labels_\nplt.figure(figsize=(12, 8))\nsns.scatterplot(X['Income'], X['Score'], hue=X['Labels'], \n                palette=sns.color_palette('hls', np.unique(db.labels_).shape[0]))\nplt.title('DBSCAN with epsilon 11, min samples 6')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Question 12:\n\n- Lancer les 3 algorithmes avec les paramètres défini précédemment\n- Pour chaque algorithmes afficher le graphique croisant \"Income\", \"Score\" avec les labels associés"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(20,15))\n\n##### KMeans #####\nax = fig.add_subplot(221)\n\nkm5 = KMeans(n_clusters=5).fit(X)\nX['Labels'] = km5.labels_\nsns.scatterplot(X['Income'], X['Score'], hue=X['Labels'], style=X['Labels'],\n                palette=sns.color_palette('hls', 5), s=60, ax=ax)\nax.set_title('KMeans with 5 Clusters')\n\n\n##### Agglomerative Clustering #####\nax = fig.add_subplot(222)\n\nagglom = AgglomerativeClustering(n_clusters=5).fit(X)\nX['Labels'] = agglom.labels_\nsns.scatterplot(X['Income'], X['Score'], hue=X['Labels'], style=X['Labels'],\n                palette=sns.color_palette('hls', 5), s=60, ax=ax)\nax.set_title('Agglomerative with 5 Clusters')\n\n\n##### DBSCAN #####\nax = fig.add_subplot(223)\n\ndb = DBSCAN(eps=11, min_samples=6).fit(X)\nX['Labels'] = db.labels_\nsns.scatterplot(X['Income'], X['Score'], hue=X['Labels'], style=X['Labels'], s=60,\n                palette=sns.color_palette('hls', np.unique(db.labels_).shape[0]), ax=ax)\nax.set_title('DBSCAN with epsilon 11, min samples 6')\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Question 13:\n\nLe coefficient de silhouette est une métrique usuelle pour évaluer la performance du clustering lorsqu'on ne connaît pas les vrais clusters.\nLe coefficient de silhouette est calculé pour chaque échantillon : \n\n$$SC = \\frac{b-a} {max(a, b)}$$\n\n- a est la distance moyenne à tous les points du même cluster\n- b est la distance moyenne à tous les autres points du cluster le plus proche.\n\nLa silhouette prend des valeurs de -1 (pire performance) à +1 (meilleure performance). Le score global est la moyenne de la silhouette.\n\nle coefficient de silhouette est une mesure de qualité d'une partition d'un ensemble de données en clustering. Pour chaque point, son coefficient de silhouette est la différence entre la distance moyenne avec les points du même groupe que lui (cohésion) et la distance moyenne avec le points des autres groupes voisins (séparation). Si cette différence est négative, le point est en moyenne plus proche du groupe voisin que du sien : il est donc mal classé. A l'inverse, si cette différence est positive, le point est en moyenne plus proche de son groupe que du groupe voisin : il est donc bien classé.\n\nLe coefficient de silhouette proprement dit est la moyenne du coefficient de silhouette pour tous les points.\n\n- Importer metrics de sklearn\n- Calculer la silhouette pour chaque algorithmes\n- Lequel choisirez vous?"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\n\nprint(\"Silhouette Coefficient Kmeans: %0.3f\" % metrics.silhouette_score(X, km5.labels_, metric='sqeuclidean'))\nprint(\"Silhouette Coefficient Agglomerative: %0.3f\" % metrics.silhouette_score(X, agglom.labels_, metric='sqeuclidean'))\nprint(\"Silhouette Coefficient DBSCAN: %0.3f\" % metrics.silhouette_score(X, db.labels_, metric='sqeuclidean'))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}